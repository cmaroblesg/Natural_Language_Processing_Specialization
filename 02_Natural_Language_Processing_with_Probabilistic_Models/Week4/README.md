# Word embeddings with neural networks

## Introduction
Learn about how word embeddings carry the semantic meaning of words, which makes them much more powerful for NLP tasks, then build your own Continuous bag-of-words model to create word embeddings from Shakespeare text.

## Learning Objectives
* Gradient descent
* One-hot vectors
* Neural networks
* Word embeddings
* Continuous bag-of-words model
* Text pre-processing
* Tokenization
* Data generators

## Labs
* [**Lab 01:** Data Preparation](./labs/C2_W4_lecture_nb_1_data_prep.ipynb)
* [**Lab 02:** Intro to CBOW](./labs/C2_W4_lecture_nb_2_intro_to_CBOW.ipynb)
* [**Lab 03** Training the CBOW](./labs/C2_W4_lecture_nb_3_training_the_CBOW.ipynb)
* [**Lab 04:** Word Embeddings](./labs/C2_W4_lecture_nb_4_word_embeddings_hands_on.ipynb)
* [**Lab 05** Word Embeddings Step by Step](./labs/C2_W4_lecture_nb_5_word_embeddings_step_by_step.ipynb)
* [**Assignment**](./labs/C2_W4_Assignment.ipynb)
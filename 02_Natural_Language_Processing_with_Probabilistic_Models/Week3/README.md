# Autocomplete and Language Models

## Introduction
Learn about how N-gram language models work by calculating sequence probabilities, then build your own autocomplete language model using a text corpus from Twitter!

## Learning Objectives
* Conditional probabilities
* Text pre-processing
* Language modeling
* Perplexity
* K-smoothing
* N-grams
* Backoff
* Tokenization

## Labs
* [**Lab 01:** Corpus preprocessing for N-grams](./labs/)
* [**Lab 02:** Building the language model](./labs/)
* [**Lab 03:** Language model generalization](./labs/)
* [**Assignment**](./labs/)
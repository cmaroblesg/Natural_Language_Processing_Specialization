# Autocomplete and Language Models

## Introduction
Learn about how N-gram language models work by calculating sequence probabilities, then build your own autocomplete language model using a text corpus from Twitter!

## Learning Objectives
* Conditional probabilities
* Text pre-processing
* Language modeling
* Perplexity
* K-smoothing
* N-grams
* Backoff
* Tokenization

## Labs
* [**Lab 01:**](./labs/)
* [**Lab 02:**](./labs/)
* [**Lab 03:**](./labs/)
* [**Assignment**](./labs/)
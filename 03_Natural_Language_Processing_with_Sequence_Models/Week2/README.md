# Recurrent Neural Networks for Language Modeling

## Introduction
Learn about the limitations of traditional language models and see how RNNs and GRUs use sequential data for text prediction. Then build your own next-word generator using a simple RNN on Shakespeare text data!

## Learning Objectives
* N-grams
* Gated recurrent units
* Recurrent neural networks

## Labs
* [**Lab 01:** Hidden State Activation](./labs/C3_W2_lecture_nb_1_Hidden_State_Activation.ipynb)
* [**Lab 02:** Vanilla RNNs, GRUs and the scan function](./labs/C3_W2_lecture_nb_2_RNNs.ipynb)
* [**Lab 03:** Working with JAX NumPy and Calculating Perplexity](./labs/C3_W2_lecture_nb_3_perplexity.ipynb)
* [**Lab 04:** Creating a GRU model using Trax](./labs/C3_W2_lecture_nb_4_GRU.ipynb)
* [**Assignment**](./labs/C3_W2_Assignment.ipynb)